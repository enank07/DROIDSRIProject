2{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enank07/DROIDSRIProject/blob/main/DROIDenankhan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey, this is the DROID project workspace! All directories have been removed for privacy reasons, so please feel free to put your own in. Have fun! For training, make sure you run on a GPU - Enan"
      ],
      "metadata": {
        "id": "i6YjZfPMkIwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XugqbS75C8kU"
      },
      "outputs": [],
      "source": [
        "#dependencies\n",
        "!pip install -qU \\\n",
        "    cellxgene-census[tiledbsoma] \\\n",
        "    scanpy anndata tiledbsoma torch torchvision\n",
        "\n",
        "!pip install -q scikit-misc\n",
        "\n",
        "#mounted drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1abl754rs6qd"
      },
      "outputs": [],
      "source": [
        "#check data\n",
        "import cellxgene_census as cxc\n",
        "import pandas as pd\n",
        "censusgarbage= \"2025-01-30\"\n",
        "idcollection = \"ddfad306-714d-4cc0-9985-d9072820c530\"\n",
        "with cxc.open_soma(census_version=censusgarbage) as census:\n",
        "    #data pull\n",
        "    ds = (\n",
        "        census[\"census_info\"][\"datasets\"]\n",
        "        .read()\n",
        "        .concat()\n",
        "        .to_pandas() #pandas switch\n",
        "        .set_index(\"soma_joinid\")\n",
        "    )\n",
        "my_ds= ds.query(\"collection_id == @idcollection\")\n",
        "print(my_ds[[\"dataset_id\", \"dataset_title\", \"dataset_total_cell_count\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MukrS1PUk581"
      },
      "outputs": [],
      "source": [
        "#data maker with AnnData\n",
        "import cellxgene_census as cxc\n",
        "import scanpy as sc\n",
        "\n",
        "censusgarbage = \"2025-01-30\"                            #good id\n",
        "DATASET_ID     = \"c7775e88-49bf-4ba2-a03b-93f00447c958\"\n",
        "\n",
        "with cxc.open_soma(census_version=censusgarbage) as census:\n",
        "    adata = cxc.get_anndata(\n",
        "        census=census,\n",
        "        organism=\"Homo sapiens\",\n",
        "        obs_value_filter=(\n",
        "            f'dataset_id == \"{DATASET_ID}\" '\n",
        "            'and is_primary_data == True'            #all canonical cell\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(adata)\n",
        "print(adata.obs.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3oUcgqSvStQ"
      },
      "outputs": [],
      "source": [
        "#data preparation\n",
        "import scanpy as sc\n",
        "\n",
        "#normalize data via log1p to remove bias between cells (sum library bias crpa)\n",
        "sc.pp.normalize_total(adata, target_sum=1e4)\n",
        "sc.pp.log1p(adata)\n",
        "adata.layers[\"log1p\"] = adata.X.copy()   #keep copy\n",
        "\n",
        "#VARY IT TO NUANCE (hyperparameters: 3000)\n",
        "sc.pp.highly_variable_genes(\n",
        "    adata, n_top_genes=3_000, layer=\"log1p\", flavor=\"seurat_v3\", subset=True\n",
        ")\n",
        "print(adata)      #check data\n",
        "\n",
        "#save drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zf5ziDfzZBd"
      },
      "outputs": [],
      "source": [
        "#build label\n",
        "import torch\n",
        "\n",
        "#label types\n",
        "cell_codes, cell_names = adata.obs[\"cell_type\"].astype(\"category\").factorize()\n",
        "y_cell = torch.tensor(cell_codes, dtype=torch.long)\n",
        "\n",
        "#binary disease label (1=COVID-19,0=healthy)\n",
        "y_disease = torch.tensor((adata.obs[\"disease\"] != \"normal\").astype(int).values,\n",
        "    dtype=torch.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUABZuLzvob"
      },
      "outputs": [],
      "source": [
        "#memory bad (4.8gb), get tensors\n",
        "X = torch.as_tensor(adata.X.toarray(), dtype=torch.float32)\n",
        "print(X.shape)          #(647366,3000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYFTttdR1KJI"
      },
      "outputs": [],
      "source": [
        "#check data counts + labels\n",
        "cell_cats = adata.obs[\"cell_type\"].astype(\"category\")\n",
        "\n",
        "n_cell_classes = cell_cats.cat.categories.size       #classes\n",
        "cell_labels    = list(cell_cats.cat.categories)      #name\n",
        "\n",
        "print(f\"{n_cell_classes} unique cell identities\")\n",
        "print(cell_labels[:10], \"...\")                       #first 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEU2zw6X1lZj"
      },
      "outputs": [],
      "source": [
        "#string list of strings you just printed\n",
        "cell_labels = list(adata.obs[\"cell_type\"].astype(\"category\").cat.categories)\n",
        "\n",
        "#write it to Drive to decode predictions later\n",
        "import json, pathlib\n",
        "label_path = pathlib.Path(\"\")\n",
        "label_path.write_text(json.dumps(cell_labels, indent=2))\n",
        "print(\"Saved label map â†’\", label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCXrT5Sd2pZ1"
      },
      "outputs": [],
      "source": [
        "#make targets\n",
        "y_cell = torch.tensor(\n",
        "    adata.obs[\"cell_type\"].astype(\"category\").cat.codes.values,\n",
        "    dtype=torch.long\n",
        ")\n",
        "y_dis  = torch.tensor(\n",
        "    (adata.obs[\"disease\"] != \"normal\").astype(int).values,\n",
        "    dtype=torch.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kdtn93D06_n"
      },
      "outputs": [],
      "source": [
        "#trainer, dual head classfieir.\n",
        "#1 input (gene expression vector), two prediction tasks: classification (46), disease/normal (binary)\n",
        "\n",
        "#EDIT PARAMETERS, REDUCE LAYERS, PLAY WITH MODEL ARCHITECTURE....\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "n_input = X.shape[1]  #3000 genes\n",
        "n_cell = 46           #46 cells\n",
        "\n",
        "class DualHeadNet(nn.Module):\n",
        "    def __init__(self, n_input, n_cell):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(n_input, 1024),\n",
        "            nn.ReLU(), nn.Dropout(0.25),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),  nn.Dropout(0.25),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),                      #keep 256-dim embedding\n",
        "        )\n",
        "        self.head_cell    = nn.Linear(256, n_cell)\n",
        "        self.head_disease = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.shared(x)\n",
        "        return self.head_cell(z), self.head_disease(z).squeeze(-1)\n",
        "\n",
        "model = DualHeadNet(n_input, n_cell).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUSpO5VV3YtA"
      },
      "outputs": [],
      "source": [
        "#loss function\n",
        "import numpy as np\n",
        "#inverse-frequency weights for 46 soft-max: class imbalance aware\n",
        "counts = np.bincount(y_cell.numpy()) #all training cells for class k\n",
        "w_cell = torch.tensor(\n",
        "    1.0 / counts * (len(counts) / (1.0 / counts).sum()), #rare classes have large number, common classes have small number\n",
        "    dtype=torch.float32\n",
        ")\n",
        "#rarer gets bigger weight\n",
        "loss_cell= nn.CrossEntropyLoss(weight=w_cell.to(model.head_cell.weight.device)) #CELOSS best for 46 classes\n",
        "loss_disease = nn.BCEWithLogitsLoss()   #binary (0,1)\n",
        "#here a class specific factor makes sure the model doesn't forget class B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9AV6J2X3mb3"
      },
      "outputs": [],
      "source": [
        "#dataset split\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "gss    = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(np.arange(adata.n_obs), groups=groups))\n",
        "\n",
        "#masks for train/val split\n",
        "train_mask = torch.from_numpy(np.isin(np.arange(adata.n_obs), train_idx))\n",
        "val_mask   = ~train_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4o0ae-90zVR"
      },
      "outputs": [],
      "source": [
        "#dataloader/preparation\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CSRDataset(Dataset):\n",
        "    def __init__(self, csr_mat, y1, y2, mask):\n",
        "        self.csr, self.y1, self.y2 = csr_mat, y1, y2\n",
        "        self.indices = torch.where(mask)[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = int(self.indices[i])\n",
        "        x   = torch.as_tensor(self.csr[idx].toarray(), dtype=torch.float32).squeeze(0)\n",
        "        return x, self.y1[idx], self.y2[idx]\n",
        "\n",
        "train_ds = CSRDataset(adata.X, y_cell, y_dis, train_mask)\n",
        "val_ds   = CSRDataset(adata.X, y_cell, y_dis, val_mask)\n",
        "train_dl = DataLoader(train_ds, batch_size=512, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=1024,               num_workers=0, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS1Uzzx74A-r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "#device, optimizer, loader\n",
        "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model    = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "scaler    = GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "#epochs\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss = corr_cell = corr_dis = nsamp = 0\n",
        "\n",
        "    for xb, yb_cell, yb_dis in loader:\n",
        "        xb, yb_cell, yb_dis = xb.to(device), yb_cell.to(device), yb_dis.to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(train), autocast(device_type=device.type):\n",
        "            logits_cell, logit_dis = model(xb)\n",
        "            loss = (\n",
        "                loss_cell(logits_cell, yb_cell) +\n",
        "                loss_disease(logit_dis, yb_dis)\n",
        "            )\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        #bookkeeping\n",
        "        batch = xb.size(0)\n",
        "        total_loss += loss.item() * batch\n",
        "        corr_cell  += (logits_cell.argmax(1) == yb_cell).sum().item()\n",
        "        corr_dis   += ((logit_dis > 0).int() == yb_dis.int()).sum().item()\n",
        "        nsamp      += batch\n",
        "\n",
        "    return (\n",
        "        total_loss / nsamp,     #avg loss\n",
        "        corr_cell  / nsamp,     #cell accuracy\n",
        "        corr_dis   / nsamp      #disease accuracy\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSAJsT9o3_SK"
      },
      "outputs": [],
      "source": [
        "\"\"\"#training loop\n",
        "EPOCHS = 20\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_acc_cell, tr_acc_dis = run_epoch(train_dl, train=True)\n",
        "    va_loss, va_acc_cell, va_acc_dis = run_epoch(val_dl,   train=False)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train loss {tr_loss:.4f}  cell {tr_acc_cell:.3f}  dis {tr_acc_dis:.3f} | \"\n",
        "        f\"val loss {va_loss:.4f}    cell {va_acc_cell:.3f}  dis {va_acc_dis:.3f}\"\n",
        "    )\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klLZA0mv9mGw"
      },
      "outputs": [],
      "source": [
        "\"\"\"import torch, pathlib\n",
        "ckpt_path = pathlib.Path(\"\")\n",
        "torch.save(model.state_dict(), ckpt_path)\n",
        "print(\"Checkpoint written to\", ckpt_path)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIbeCWCT0eLl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "model = DualHeadNet(n_input,n_cell)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(Path(\"\")))\n",
        "model.eval()\n",
        "print(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3tEltxmDxdf"
      },
      "source": [
        "# **Model training is done, the rest are diagnostics!** - Enan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hq8ridDFBUW"
      },
      "source": [
        "Confusion matrix + ROC CURVE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDfuTlSL_gzv"
      },
      "outputs": [],
      "source": [
        "#TESTING + DIAGNOSTICS\n",
        "\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = model.to(device).eval()\n",
        "\n",
        "#20% test split\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "idx    = np.arange(adata.n_obs)\n",
        "gss = GroupShuffleSplit(test_size=0.20, random_state=123)\n",
        "_, test_idx = next(gss.split(idx, groups=groups))\n",
        "test_mask = np.isin(idx, test_idx)\n",
        "test_mask = torch.from_numpy(test_mask)\n",
        "test_ds = CSRDataset(adata.X, y_cell, y_dis, test_mask)\n",
        "test_dl = DataLoader(test_ds, batch_size=512, shuffle=False,\n",
        "                     num_workers=0, pin_memory=True)\n",
        "#inference/predictions\n",
        "pred_cell, true_cell = [], []\n",
        "pred_dis , true_dis  = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb_cell, yb_dis in test_dl:\n",
        "        xb = xb.to(device)\n",
        "        logits_cell, logits_dis = model(xb)\n",
        "        pred_cell.append(logits_cell.argmax(1).cpu())\n",
        "        true_cell.append(yb_cell)\n",
        "        pred_dis.append(torch.sigmoid(logits_dis).cpu())\n",
        "        true_dis.append(yb_dis)\n",
        "pred_cell = torch.cat(pred_cell).numpy()\n",
        "true_cell = torch.cat(true_cell).numpy()\n",
        "pred_dis  = torch.cat(pred_dis ).numpy()\n",
        "true_dis  = torch.cat(true_dis ).numpy()\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(true_cell, pred_cell, labels=np.arange(len(np.unique(y_cell))))\n",
        "fig_cm, ax_cm = plt.subplots(figsize=(7, 7))\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=np.arange(46))\n",
        "disp.plot(ax=ax_cm, include_values=False, xticks_rotation=90, cmap=\"Blues\")\n",
        "ax_cm.set_title(\"Confusion Matrix (Test) â€“ Cell-Type Head\")\n",
        "plt.tight_layout()\n",
        "#roc curve\n",
        "fpr, tpr, _ = roc_curve(true_dis, pred_dis)\n",
        "roc_auc     = auc(fpr, tpr)\n",
        "\n",
        "fig_roc, ax_roc = plt.subplots()\n",
        "ax_roc.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "ax_roc.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")\n",
        "ax_roc.set_xlabel(\"False Positive Rate\")\n",
        "ax_roc.set_ylabel(\"True Positive Rate\")\n",
        "ax_roc.set_title(\"ROC Curve (Test) â€“ Disease Head\")\n",
        "ax_roc.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCKIlDG8F4U0"
      },
      "source": [
        "**AUPR CURVE IS A BETTER METRIC SINCE DATA IS UNBALANCED SO HERE IT IS!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L5BEJZEF0OO"
      },
      "outputs": [],
      "source": [
        "#reimport for standalone\n",
        "import numpy as np, torch, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "\n",
        "#indices\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "idx    = np.arange(adata.n_obs)\n",
        "_, test_idx = next(GroupShuffleSplit(test_size=0.20, random_state=123)\n",
        "                   .split(idx, groups=groups))\n",
        "test_mask = torch.from_numpy(np.isin(idx, test_idx))\n",
        "\n",
        "#wrap CSR dataset to remove labels\n",
        "class NoLabels(Dataset):\n",
        "    def __init__(self, base_ds):\n",
        "        self.base = base_ds\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, *_ = self.base[i]     #keeps only feature vector\n",
        "        return x\n",
        "\n",
        "test_ds = NoLabels(CSRDataset(adata.X, y_cell, y_dis, test_mask))\n",
        "test_dl = DataLoader(test_ds, batch_size=2048, shuffle=False, num_workers=0)\n",
        "#get probabilities\n",
        "y_prob = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_dl:\n",
        "        xb = xb.to(device)\n",
        "        z  = model.shared(xb)\n",
        "        logits_d = model.head_disease(z).squeeze(-1)\n",
        "        y_prob.append(torch.sigmoid(logits_d).cpu().numpy())\n",
        "y_prob = np.concatenate(y_prob)\n",
        "\n",
        "y_true = (adata.obs[\"disease\"].values[test_idx] != \"normal\").astype(int)\n",
        "\n",
        "#precision recall + AUPR\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "aupr = average_precision_score(y_true, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall, precision, lw=2, label=f\"AUPR = {aupr:.3f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precisionâ€“Recall curve â€“ disease head\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o7_3irED39K"
      },
      "source": [
        "**UMAPS ARE NEXT!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX7Nk0UqEFsG"
      },
      "source": [
        "**Disease features**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPpg5JzhYbQk"
      },
      "source": [
        "**Penultimate layer from the model into making the umap**\n",
        "- Raw counts, use the MLP\n",
        "- They have a shared embedding; take those values on UMAP.\n",
        "- Latent features!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWfDtg_5Kx80"
      },
      "outputs": [],
      "source": [
        "#steaming + low ram umap\n",
        "import torch, numpy as np, scanpy as sc, matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "\n",
        "# chaooose 50 k random cells\n",
        "n_vis= 50_000\n",
        "subset = np.random.choice(adata.n_obs, n_vis, replace=False)\n",
        "\n",
        "#latent space = 256 features\n",
        "z_dim  = 256                      #penultimate layer size\n",
        "latent= np.empty((n_vis, z_dim), dtype=\"float32\")\n",
        "logits_d= np.empty(n_vis,          dtype=\"float32\")\n",
        "\n",
        "#steam in 1k cell minibatches\n",
        "BATCH = 1_000\n",
        "for i in tqdm(range(0, n_vis, BATCH), desc=\"Embedding\", unit_scale=BATCH):\n",
        "    idx  = subset[i:i+BATCH]\n",
        "    X_np = adata.X[idx].toarray().astype(\"float32\")          #ram bad\n",
        "    xb   = torch.tensor(X_np, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z      = model.shared(xb)                 #whatever batch, x 256\n",
        "        logit  = model.head_disease(z).squeeze(1) #THIS IS TWHAT THE MODEL SEES\n",
        "\n",
        "    latent[i:i+BATCH]   = z.cpu().numpy()\n",
        "    logits_d[i:i+BATCH] = logit.cpu().numpy()\n",
        "\n",
        "#ANN DATA NOW BUILT ON LATENT DATA FEATURES\n",
        "adata_lat = sc.AnnData(latent)\n",
        "pred_dis  = (torch.sigmoid(torch.tensor(logits_d)) > 0.5).numpy().astype(int)\n",
        "true_dis  = (adata.obs[\"disease\"] != \"normal\").astype(int).values[subset]\n",
        "\n",
        "adata_lat.obs[\"disease_true\"] = np.where(true_dis, \"COVID-19\", \"normal\")\n",
        "adata_lat.obs[\"disease_pred\"] = np.where(pred_dis,  \"COVID-19\", \"normal\")\n",
        "adata_lat.obs[\"correct\"]      = np.where(true_dis == pred_dis, \"correct\", \"wrong\")\n",
        "\n",
        "#umap\n",
        "sc.pp.neighbors(adata_lat, n_neighbors=15, use_rep=\"X\")\n",
        "sc.tl.umap(adata_lat, min_dist=0.3)\n",
        "#color palleteeu\n",
        "disease_palette = [\"#1f77b4\", \"#ff7f0e\"]                #blue/ orange\n",
        "correct_palette = {\"correct\": \"#2ca02c\", \"wrong\": \"#d62728\"}  #green / red\n",
        "\n",
        "#3 panel plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"disease_true\", palette=disease_palette,\n",
        "    title=\"UMAP â€“ disease ground truth\", legend_loc='right margin',\n",
        "    frameon=False, size=6, ax=axs[0], show=False\n",
        ")\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"disease_pred\", palette=disease_palette,\n",
        "    title=\"UMAP â€“ disease model predictions\", legend_loc='right margin',\n",
        "    frameon=False, size=6, ax=axs[1], show=False\n",
        ")\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"correct\",\n",
        "    palette=[correct_palette[\"correct\"], correct_palette[\"wrong\"]],\n",
        "    title=\"Prediction correctness (green = right, red = wrong)\",\n",
        "    legend_loc='right margin', frameon=False, size=6, ax=axs[2], show=False\n",
        ")\n",
        "\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqVI3o_uRRAQ"
      },
      "source": [
        "**Gene feature space instead, on different embeddings!** This is still the disease head, but the gene network is kept the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZTaPOvhCwKK"
      },
      "outputs": [],
      "source": [
        "#disease predictions individual on gene feature space!\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "#sample same data\n",
        "n_vis = 50_000\n",
        "subset = np.random.choice(adata.n_obs, n_vis, replace=False)\n",
        "\n",
        "gene_mat = adata.X[subset].toarray().astype(\"float32\")  #(50k,3000)\n",
        "\n",
        "adata_gene = sc.AnnData(gene_mat)\n",
        "sc.pp.scale(adata_gene, max_value=10, zero_center=False)\n",
        "sc.tl.pca(adata_gene, n_comps=50, svd_solver=\"randomized\")\n",
        "sc.pp.neighbors(adata_gene, n_pcs=50, n_neighbors=15)\n",
        "sc.tl.umap(adata_gene, min_dist=0.3)\n",
        "\n",
        "#disease head needed\n",
        "with torch.no_grad():\n",
        "    xb = torch.tensor(gene_mat, dtype=torch.float32, device=device)\n",
        "    logit_d = model(xb)[1]\n",
        "    pred_dis = (torch.sigmoid(logit_d) > 0.5).cpu().numpy().astype(int)\n",
        "\n",
        "true_dis = (adata.obs[\"disease\"] != \"normal\").astype(int).values[subset]  #1= COVID, 0 =normal\n",
        "\n",
        "adata_gene.obs[\"disease_true\"] = np.where(true_dis, \"COVID-19\", \"normal\")\n",
        "adata_gene.obs[\"disease_pred\"] = np.where(pred_dis,  \"COVID-19\", \"normal\")\n",
        "adata_gene.obs[\"correct\"]      = np.where(true_dis == pred_dis, \"correct\", \"wrong\")\n",
        "\n",
        "#binary palette for color correctness/incorrecetmeness\n",
        "disease_palette = [\"#1f77b4\", \"#ff7f0e\"] #blue,orange\n",
        "correct_palette = {\"correct\": \"#2ca02c\", \"wrong\": \"#d62728\"}#green/red\n",
        "\n",
        "#plot each\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_gene,\n",
        "    color=\"disease_true\",\n",
        "    palette=disease_palette,\n",
        "    title=\"UMAP â€“ disease ground truth\",\n",
        "    legend_loc='right margin',\n",
        "    frameon=False,\n",
        "    size=6,\n",
        "    ax=axs[0],\n",
        "    show=False\n",
        ")\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_gene,\n",
        "    color=\"disease_pred\",\n",
        "    palette=disease_palette,\n",
        "    title=\"UMAP â€“ disease model predictions\",\n",
        "    legend_loc='right margin',\n",
        "    frameon=False,\n",
        "    size=6,\n",
        "    ax=axs[1],\n",
        "    show=False\n",
        ")\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_gene,\n",
        "    color=\"correct\",\n",
        "    palette=[correct_palette[\"correct\"], correct_palette[\"wrong\"]],\n",
        "    title=\"Prediction correctness (greenÂ =Â right, redÂ =Â wrong)\",\n",
        "    legend_loc='right margin',\n",
        "    frameon=False,\n",
        "    size=6,\n",
        "    ax=axs[2],\n",
        "    show=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pqBuhAOECts"
      },
      "source": [
        "**Cell feature space**. For cell detection!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbUVdLUyD85L"
      },
      "source": [
        "**Latent space, this is what the model labels!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrX0wlCxROfI"
      },
      "outputs": [],
      "source": [
        "#latent\n",
        "!pip install -q scikit-learn tqdm\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#right device\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "\n",
        "#50k random\n",
        "n_vis  = 50_000\n",
        "subset = np.random.choice(adata.n_obs, n_vis, replace=False)\n",
        "\n",
        "#preallocate arrays for 256-dim latent and predicted labels\n",
        "latent    = np.empty((n_vis, 256), dtype=\"float32\")\n",
        "pred_lab  = np.empty(n_vis,       dtype=int)\n",
        "\n",
        "#1k rows so code doesnt die\n",
        "BATCH = 1_000\n",
        "for i in tqdm(range(0, n_vis, BATCH), desc=\"Embedding\", unit_scale=BATCH):\n",
        "    idx  = subset[i:i+BATCH]\n",
        "    X_np = adata.X[idx].toarray().astype(\"float32\")\n",
        "    xb   = torch.tensor(X_np, device=device)\n",
        "    with torch.no_grad():\n",
        "        z      = model.shared(xb)                 #batch + 256\n",
        "        logits = model.head_cell(z)               #batch + 46\n",
        "    latent[i:i+BATCH]   = z.cpu().numpy()\n",
        "    pred_lab[i:i+BATCH] = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "#ann data on LATENT SPACE\n",
        "adata_lat = sc.AnnData(latent)\n",
        "\n",
        "#true label\n",
        "true_lab = pd.Categorical(adata.obs[\"cell_type\"]\n",
        "                         .values[subset]).codes\n",
        "\n",
        "#store categorical code\n",
        "adata_lat.obs[\"true_code\"] = true_lab\n",
        "adata_lat.obs[\"pred_code\"] = pred_lab\n",
        "adata_lat.obs[\"correct\"]   = np.where(true_lab == pred_lab,\n",
        "                                     \"correct\", \"wrong\")\n",
        "\n",
        "#compute neighbors and UMAP\n",
        "sc.pp.neighbors(adata_lat, n_neighbors=15, use_rep=\"X\")\n",
        "sc.tl.umap(adata_lat, min_dist=0.3)\n",
        "\n",
        "#color palettee\n",
        "cell_labels = adata.obs[\"cell_type\"].astype(\"category\").cat.categories.tolist()\n",
        "palette     = sc.pl.palettes.default_102[:len(cell_labels)]\n",
        "correct_pal = {\"correct\": \"#2ca02c\", \"wrong\": \"#d62728\"}\n",
        "\n",
        "#side-by-side UMAPs\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"true_code\", palette=palette,\n",
        "    title=\"UMAP (latent) â€“ ground truth\", legend_loc=\"on data\",\n",
        "    frameon=False, size=6, ax=axs[0], show=False\n",
        ")\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"pred_code\", palette=palette,\n",
        "    title=\"UMAP (latent) â€“ model prediction\", legend_loc=\"on data\",\n",
        "    frameon=False, size=6, ax=axs[1], show=False\n",
        ")\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"correct\",\n",
        "    palette=[correct_pal[\"correct\"], correct_pal[\"wrong\"]],\n",
        "    title=\"Prediction correctness\", legend_loc=\"right margin\",\n",
        "    frameon=False, size=6, ax=axs[2], show=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#display code to cell type mapping for reference\n",
        "mapping = pd.DataFrame({\n",
        "    \"code\": range(len(cell_labels)),\n",
        "    \"cell_type\": cell_labels\n",
        "})\n",
        "display(mapping)\n",
        "\n",
        "#print out classification report\n",
        "print(\"Classification Report on subset:\")\n",
        "print(classification_report(true_lab, pred_lab, target_names=cell_labels, digits=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2RbIkX4a0Y"
      },
      "source": [
        "LATENT SPACE COLORING NON-GRADIENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkek7Ugl4fc6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import scanpy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#config\n",
        "N_VIS     = 50_000\n",
        "LAT_DIM   = 256\n",
        "BATCH     = 1_000\n",
        "MIN_DIST  = 0.3\n",
        "KEY_CSV   = \"celltype_colour_key.csv\"   #most convenient way for keeping color labels\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "\n",
        "subset_idx = np.random.choice(adata.n_obs, N_VIS, replace=False)\n",
        "\n",
        "latent   = np.empty((N_VIS, LAT_DIM), dtype=np.float32)\n",
        "pred_lab = np.empty(N_VIS,               dtype=np.int32)\n",
        "\n",
        "for start in tqdm(range(0, N_VIS, BATCH), desc=\"Embedding\", unit=\"cells\"):\n",
        "    end = start + BATCH\n",
        "    idx = subset_idx[start:end]\n",
        "\n",
        "    xb = torch.tensor(adata.X[idx].toarray(),\n",
        "                      dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z      = model.shared(xb)        #(batch Ã— 256)\n",
        "        logits = model.head_cell(z)      #(batch Ã— n_celltypes)\n",
        "\n",
        "    latent[start:end]   = z.cpu().numpy()\n",
        "    pred_lab[start:end] = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "#ram fix\n",
        "del xb, z, logits\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#build ann data in latent space\n",
        "adata_lat = sc.AnnData(latent)           #vis x 256\n",
        "\n",
        "true_lab = pd.Categorical(\n",
        "    adata.obs[\"cell_type\"].values[subset_idx]\n",
        ").codes\n",
        "\n",
        "adata_lat.obs[\"true_code\"] = pd.Categorical(true_lab)\n",
        "adata_lat.obs[\"pred_code\"] = pd.Categorical(pred_lab)\n",
        "adata_lat.obs[\"correct\"]   = np.where(true_lab == pred_lab, \"correct\", \"wrong\")\n",
        "\n",
        "sc.pp.neighbors(adata_lat, n_neighbors=15, use_rep=\"X\")\n",
        "sc.tl.umap(adata_lat, min_dist=MIN_DIST)\n",
        "\n",
        "cell_labels = (\n",
        "    adata.obs[\"cell_type\"]\n",
        "    .astype(\"category\")\n",
        "    .cat.categories\n",
        "    .tolist()\n",
        ")\n",
        "palette     = sc.pl.palettes.default_102[:len(cell_labels)]  #â‰¤102 unique hues\n",
        "correct_pal = {\"correct\": \"#2ca02c\", \"wrong\": \"#d62728\"}\n",
        "\n",
        "#print all umaps\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"true_code\", palette=palette,\n",
        "    title=\"Latent UMAP â€“ ground truth\",\n",
        "    legend_loc=None, frameon=False, size=6, ax=axs[0], show=False\n",
        ")\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"pred_code\", palette=palette,\n",
        "    title=\"Latent UMAP â€“ model prediction\",\n",
        "    legend_loc=None, frameon=False, size=6, ax=axs[1], show=False\n",
        ")\n",
        "sc.pl.umap(\n",
        "    adata_lat, color=\"correct\",\n",
        "    palette=[correct_pal[\"correct\"], correct_pal[\"wrong\"]],\n",
        "    title=\"Prediction correctness\",\n",
        "    legend_loc=\"right margin\", frameon=False, size=6, ax=axs[2], show=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "colour_key = pd.DataFrame({\n",
        "    \"code\"      : range(len(cell_labels)),\n",
        "    \"cell_type\" : cell_labels,\n",
        "    \"colour_hex\": palette\n",
        "})\n",
        "display(colour_key)\n",
        "\n",
        "colour_key.to_csv(KEY_CSV, index=False)\n",
        "print(f\"\\nSaved colour key â†’ {KEY_CSV}\")\n",
        "\n",
        "print(\"\\nClassification report on the sampled subset:\")\n",
        "print(classification_report(true_lab, pred_lab,\n",
        "                            target_names=cell_labels, digits=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qCQ13t3TSIk"
      },
      "source": [
        "Gene expression instead, raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HafTbhxfAdGg"
      },
      "outputs": [],
      "source": [
        "#cell feature space on gene expression matrix, not latent features\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "#50k subset\n",
        "n_vis = 50_000\n",
        "subset = np.random.choice(adata.n_obs, n_vis, replace=False)\n",
        "\n",
        "#build gene expression matrix\n",
        "gene_mat = adata.X[subset].toarray().astype(\"float32\")\n",
        "\n",
        "#run pca\n",
        "adata_gene = sc.AnnData(gene_mat)\n",
        "sc.pp.scale(adata_gene, max_value=10, zero_center=False)\n",
        "sc.tl.pca(adata_gene, n_comps=50, svd_solver=\"randomized\")\n",
        "sc.pp.neighbors(adata_gene, n_pcs=50, n_neighbors=15)\n",
        "sc.tl.umap(adata_gene, min_dist=0.3)\n",
        "\n",
        "#true+predicted labels\n",
        "with torch.no_grad():\n",
        "    xb = torch.tensor(gene_mat, device=device)\n",
        "    logits = model(xb)[0]              #cell-type head\n",
        "    pred_lab = logits.argmax(1).cpu().numpy()\n",
        "\n",
        "true_lab = pd.Categorical(adata.obs[\"cell_type\"]).codes[subset]\n",
        "\n",
        "adata_gene.obs[\"true_code\"] = true_lab\n",
        "adata_gene.obs[\"pred_code\"] = pred_lab\n",
        "\n",
        "#colour palette\n",
        "palette = sc.pl.palettes.default_102[: len(cell_labels)]\n",
        "\n",
        "#plot side-by-side on same embedding\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sc.pl.umap(adata_gene, color=\"true_code\", palette=palette, size=6,\n",
        "           legend_loc=None, frameon=False, ax=axs[0], show=False)\n",
        "axs[0].set_title(\"Gene UMAP â€“ ground truth\")\n",
        "sc.pl.umap(adata_gene, color=\"pred_code\", palette=palette, size=6,\n",
        "           legend_loc=None, frameon=False, ax=axs[1], show=False)\n",
        "axs[1].set_title(\"Gene UMAP â€“ model prediction\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "#per accuracy table\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_lab, pred_lab, target_names=cell_labels, digits=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX-anTC3IeuG"
      },
      "source": [
        "#SHAP VALUES - PART 3: interpreting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTBJ1YUkBSXe"
      },
      "source": [
        "**DISEASE SHAP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfPCdiX9Tunx"
      },
      "source": [
        "**MAIN DISEASE SHAP CODE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpGZo0oUXfZM"
      },
      "outputs": [],
      "source": [
        "#DISEASE SHAP\n",
        "!pip install -q shap tqdm\n",
        "import shap, torch, numpy as np, matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import warnings, gc\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"shap\")\n",
        "\n",
        "#put on CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "disease_mod = torch.nn.Sequential(model.shared.to(device),\n",
        "                                  model.head_disease.to(device)).eval()\n",
        "\n",
        "#1000 cells\n",
        "bg_idx  = np.random.choice(train_idx, 1_000, replace=False)\n",
        "bg_data = torch.tensor(\n",
        "    adata.X[bg_idx].toarray().astype(\"float32\"), device=device\n",
        ")\n",
        "explainer = shap.DeepExplainer(disease_mod, bg_data)\n",
        "\n",
        "#8000 random test clles\n",
        "TOTAL, BATCH = 8_000, 128\n",
        "exp_idx_all  = np.random.choice(test_idx, TOTAL, replace=False)\n",
        "\n",
        "rows = []\n",
        "for s in tqdm(range(0, TOTAL, BATCH), desc=\"Explaining\", unit_scale=BATCH):\n",
        "    idx  = exp_idx_all[s:s+BATCH]\n",
        "    X_np = adata.X[idx].toarray().astype(\"float32\")\n",
        "    xb   = torch.tensor(X_np, device=device)\n",
        "\n",
        "    sv = explainer.shap_values(xb)[0]      #listâ†’array\n",
        "    sv = np.squeeze(sv)\n",
        "    if sv.ndim == 1:\n",
        "        sv = np.tile(sv, (X_np.shape[0], 1))\n",
        "    if sv.shape != X_np.shape:\n",
        "        sv = sv.T\n",
        "    rows.append(sv)\n",
        "    del xb; gc.collect()\n",
        "\n",
        "shap_vals = np.vstack(rows)                #(8000,genes)\n",
        "print(\"SHAP matrix:\", shap_vals.shape)\n",
        "\n",
        "#mean + SHAP VALUES\n",
        "mean_abs = np.mean(np.abs(shap_vals), axis=0)\n",
        "top_k    = 30\n",
        "top_idx  = np.argsort(-mean_abs)[:top_k]\n",
        "\n",
        "#map indices\n",
        "symbol_col = \"feature_name\" if \"feature_name\" in adata.var.columns else None\n",
        "symbols = adata.var[symbol_col].tolist() if symbol_col else adata.var_names.tolist()\n",
        "gene_labels = [symbols[i] for i in top_idx]\n",
        "\n",
        "#bar plot\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.barh(range(top_k)[::-1], mean_abs[top_idx][::-1])\n",
        "plt.yticks(range(top_k)[::-1], gene_labels[::-1])\n",
        "plt.xlabel(\"Mean |SHAP value|\")\n",
        "plt.title(\"Disease head â€“ top 30 genes\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "#beeswarm data\n",
        "X_feat = adata.X[exp_idx_all].toarray()[:, top_idx]\n",
        "shap.summary_plot(\n",
        "    shap_vals[:, top_idx],\n",
        "    features=X_feat,\n",
        "    feature_names=gene_labels,\n",
        "    max_display=top_k,\n",
        "    show=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgdMAhEeBZtx"
      },
      "outputs": [],
      "source": [
        "idx_to_gene = adata.var['feature_name'].tolist()   #or .index if names stored there\n",
        "important_genes = [idx_to_gene[i] for i in top_idx]\n",
        "print(\"Top genes:\", important_genes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1d-wmcgB7b8"
      },
      "source": [
        "**CELL HEAD SHAP**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXcMXwbTB_z0"
      },
      "outputs": [],
      "source": [
        "#shap analysis\n",
        "!pip install -q shap tqdm\n",
        "import shap, torch, numpy as np, matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import warnings, gc\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"shap\")\n",
        "\n",
        "#choose class index\n",
        "cell_type_idx = 1    #Change from 1-45 as needed\n",
        "cell_name = adata.obs[\"cell_type\"].cat.categories[cell_type_idx]\n",
        "print(f\"Explaining class {cell_type_idx}: {cell_name}\")\n",
        "\n",
        "#class to remove logits\n",
        "class SingleClass(torch.nn.Module):\n",
        "    def __init__(self, shared, head, idx):\n",
        "        super().__init__()\n",
        "        self.shared, self.head, self.idx = shared, head, idx\n",
        "    def forward(self, x):\n",
        "        return self.head(self.shared(x))[:, self.idx].unsqueeze(1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_one = SingleClass(model.shared.to(device),\n",
        "                        model.head_cell.to(device),\n",
        "                        cell_type_idx).eval()\n",
        "\n",
        "#1000 random training cells\n",
        "bg_idx  = np.random.choice(train_idx, 1_000, replace=False)\n",
        "bg_data = torch.tensor(\n",
        "    adata.X[bg_idx].toarray().astype(\"float32\"), device=device\n",
        ")\n",
        "explainer = shap.DeepExplainer(model_one, bg_data)\n",
        "\n",
        "#3000 is max, if less, it uses less cells\n",
        "class_rows = np.where(y_cell.numpy() == cell_type_idx)[0]\n",
        "MAX_CELLS  = 3_000\n",
        "if len(class_rows) > MAX_CELLS:\n",
        "    class_rows = np.random.choice(class_rows, MAX_CELLS, replace=False)\n",
        "TOTAL = len(class_rows)\n",
        "print(f\"Using {TOTAL} cells (of {np.sum(y_cell.numpy()==cell_type_idx)})\")\n",
        "\n",
        "#128 mini cell batches\n",
        "BATCH = 128\n",
        "rows  = []\n",
        "for s in tqdm(range(0, TOTAL, BATCH), desc=\"Explaining\", unit_scale=BATCH):\n",
        "    idx  = class_rows[s:s+BATCH]\n",
        "    X_np = adata.X[idx].toarray().astype(\"float32\")\n",
        "    xb   = torch.tensor(X_np, device=device)\n",
        "\n",
        "    sv = explainer.shap_values(xb)[0]\n",
        "    sv = np.squeeze(sv)\n",
        "    if sv.ndim == 1:\n",
        "        sv = np.tile(sv, (X_np.shape[0], 1))\n",
        "    if sv.shape != X_np.shape:\n",
        "        sv = sv.T\n",
        "    rows.append(sv)\n",
        "    del xb; gc.collect()\n",
        "\n",
        "shap_vals = np.vstack(rows)\n",
        "print(\"SHAP matrix:\", shap_vals.shape)\n",
        "\n",
        "#top 30 mean genes\n",
        "mean_abs = np.mean(np.abs(shap_vals), axis=0)\n",
        "top_k    = 30\n",
        "top_idx  = np.argsort(-mean_abs)[:top_k]\n",
        "\n",
        "#gene symbol for indices\n",
        "symbol_col = \"feature_name\" if \"feature_name\" in adata.var.columns else None\n",
        "symbols = adata.var[symbol_col].tolist() if symbol_col else adata.var_names.tolist()\n",
        "gene_labels = [symbols[i] for i in top_idx]\n",
        "\n",
        "#bar plot\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.barh(range(top_k)[::-1], mean_abs[top_idx][::-1])\n",
        "plt.yticks(range(top_k)[::-1], gene_labels[::-1])\n",
        "plt.xlabel(\"Mean |SHAP value|\")\n",
        "plt.title(f\"{cell_name} â€“ top {top_k} genes\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "#beeswarm design\n",
        "X_feat = adata.X[class_rows].toarray()[:, top_idx]\n",
        "shap.summary_plot(\n",
        "    shap_vals[:, top_idx],\n",
        "    features=X_feat,\n",
        "    feature_names=gene_labels,\n",
        "    max_display=top_k,\n",
        "    show=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBmhB_0ZaHNa"
      },
      "source": [
        "DOT PLOT FOR GENE EXPRESSION: QUANTITATIVE\n",
        "- Permutation of models: zero out of the genes that are important, and see how it changes the model's prediction.\n",
        "- See if prediction accuracy increases or decreases\n",
        "- Compare and contrast with confusion matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6CFZz1KMqiD"
      },
      "source": [
        "ANALYSIS VIA GRAPHS AND DOTPLOTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleClass(torch.nn.Module):\n",
        "    def __init__(self, shared, head, idx):\n",
        "        super().__init__()\n",
        "        self.s, self.h, self.i = shared, head, idx\n",
        "    def forward(self, x):\n",
        "        return self.h(self.s(x))[:, self.i].unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "1QkBm62XW0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, gc, numpy as np, torch, shap\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "CACHE_FILE = \"/content/top5_shap_by_class.pkl\"\n",
        "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BG_SIZE     = 100     #background cells\n",
        "EXPL_CELLS  = 150     #cells explained per class\n",
        "TOP_K       = 5\n",
        "\n",
        "if os.path.exists(CACHE_FILE):\n",
        "    with open(CACHE_FILE, \"rb\") as f:\n",
        "        top5_by_class = pickle.load(f)\n",
        "    print(\"Loaded cached SHAP dictionary.\")\n",
        "else:\n",
        "    train_idx   = np.setdiff1d(np.arange(adata.n_obs), test_idx)\n",
        "    gene_labels = (adata.var[\"feature_name\"]\n",
        "                   if \"feature_name\" in adata.var.columns\n",
        "                   else adata.var_names).tolist()\n",
        "\n",
        "    n_cell = len(adata.obs[\"cell_type\"].cat.categories)\n",
        "    top5_by_class = {}\n",
        "\n",
        "    #wrapper returning shape (batch,1)\n",
        "    class SingleClass(torch.nn.Module):\n",
        "        def __init__(self, shared, head, idx):\n",
        "            super().__init__()\n",
        "            self.s, self.h, self.i = shared, head, idx\n",
        "        def forward(self, x):\n",
        "            return self.h(self.s(x))[:, self.i].unsqueeze(1)\n",
        "\n",
        "    for cls in tqdm(range(n_cell)):\n",
        "        bg_idx  = np.random.choice(train_idx, BG_SIZE, replace=False)\n",
        "        bg_data = torch.tensor(\n",
        "            adata.X[bg_idx].toarray().astype(\"float32\"), device=DEVICE)\n",
        "\n",
        "        explainer = shap.DeepExplainer(\n",
        "            SingleClass(model.shared.to(DEVICE),\n",
        "                        model.head_cell.to(DEVICE),\n",
        "                        cls).eval(),\n",
        "            bg_data)\n",
        "\n",
        "        rows = np.where(y_cell.numpy() == cls)[0]\n",
        "        rows = np.random.choice(rows, min(EXPL_CELLS, len(rows)), False)\n",
        "        xb = torch.tensor(\n",
        "            adata.X[rows].toarray().astype(\"float32\"), device=DEVICE)\n",
        "\n",
        "        sv = explainer.shap_values(xb)[0]          #(rows, genes)\n",
        "        if sv.ndim == 1:\n",
        "          sv = sv.reshape(1, -1)\n",
        "        elif sv.shape[0] != xb.shape[0]:\n",
        "          sv = sv.T\n",
        "        mean_abs = np.mean(np.abs(sv), axis=0)\n",
        "        top_idx  = np.argsort(-mean_abs)[:TOP_K]\n",
        "        top5_by_class[cls] = adata.var_names[top_idx].tolist()\n",
        "\n",
        "        del explainer, bg_data, xb, sv; gc.collect()\n",
        "\n",
        "    with open(CACHE_FILE, \"wb\") as f:\n",
        "        pickle.dump(top5_by_class, f)\n",
        "    print(\"Saved SHAP markers â†’\", CACHE_FILE)\n"
      ],
      "metadata": {
        "id": "LwaA-zM4RIIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(top5_by_class.items())[:3])\n"
      ],
      "metadata": {
        "id": "cEk9IkJvY0U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actually plot\n",
        "import math, pickle, scanpy as sc, matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"/content/top5_shap_by_class.pkl\", \"rb\") as f:\n",
        "    top5_by_class = pickle.load(f)\n",
        "\n",
        "GROUP_KEY    = \"cell_type\"\n",
        "all_types    = adata.obs[GROUP_KEY].cat.categories\n",
        "ROWS_PER_PAGE = 10\n",
        "n_pages      = math.ceil(len(all_types) / ROWS_PER_PAGE)\n",
        "\n",
        "for page in range(n_pages):\n",
        "    lo, hi         = page * ROWS_PER_PAGE, (page + 1) * ROWS_PER_PAGE\n",
        "    rows_on_page   = all_types[lo:hi]\n",
        "    adata_pg       = adata[adata.obs[GROUP_KEY].isin(rows_on_page)].copy()\n",
        "\n",
        "    ordered_ids = []\n",
        "    for ct in rows_on_page:\n",
        "        cls_idx = list(all_types).index(ct)\n",
        "        ordered_ids.extend(top5_by_class[cls_idx])\n",
        "\n",
        "    ordered_ids = [gid for gid in ordered_ids if gid in adata_pg.var_names]\n",
        "    if not ordered_ids:\n",
        "        print(f\"Page {page+1}: no matching genes, skipped.\")\n",
        "        continue\n",
        "\n",
        "    sc.pl.dotplot(\n",
        "        adata_pg,\n",
        "        var_names=ordered_ids,\n",
        "        groupby=GROUP_KEY,\n",
        "        standard_scale=\"var\",\n",
        "        figsize=(20, 4 + 0.3 * len(rows_on_page)),\n",
        "        show=False\n",
        "    )\n",
        "    plt.suptitle(\n",
        "        f\"Modelâ€‘derived Topâ€‘5 SHAP Genes per Cell Type (Page {page+1}/{n_pages})\",\n",
        "        y=1.03, fontsize=16\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DMclWrCTRPgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, pickle\n",
        "\n",
        "# load the cached perâ€‘class dictionary\n",
        "with open(\"/content/top5_shap_by_class.pkl\", \"rb\") as f:\n",
        "    top5_by_class = pickle.load(f)\n",
        "\n",
        "flat_ids, seen = [], set()\n",
        "for id_list in top5_by_class.values():\n",
        "    for gid in id_list:\n",
        "        if gid not in seen:\n",
        "            flat_ids.append(gid)\n",
        "            seen.add(gid)\n",
        "\n",
        "if \"feature_name\" in adata.var.columns:\n",
        "    id2sym = adata.var[\"feature_name\"].to_dict()\n",
        "else:                #symbols already in var_names\n",
        "    id2sym = {g: g for g in adata.var_names}\n",
        "\n",
        "table = pd.DataFrame({\n",
        "    \"GeneÂ ID\": flat_ids,\n",
        "    \"GeneÂ Symbol\": [id2sym.get(g, g) for g in flat_ids]\n",
        "})\n",
        "\n",
        "# prettyâ€‘print\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "print(table.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "vnD0FBiMc_De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2JrYJpiLhWx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"scanpy\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scanpy\")\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "\n",
        "#20k sample for prediction\n",
        "N_SAMP = 20_000\n",
        "subset_idx = np.random.choice(adata.n_obs, N_SAMP, replace=False)\n",
        "X_samp = adata.X[subset_idx].toarray().astype(\"float32\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    xb = torch.tensor(X_samp, device=device)\n",
        "    z = model.shared(xb)\n",
        "    disease_logits = model.head_disease(z).squeeze(1).cpu()\n",
        "    predicted_disease = (torch.sigmoid(disease_logits) > 0.5).numpy().astype(int)\n",
        "\n",
        "adata_sub = sc.AnnData(X_samp)\n",
        "adata_sub.var_names = adata.var_names\n",
        "adata_sub.var['feature_name'] = adata.var['feature_name'].values\n",
        "adata_sub.obs[\"disease_pred\"] = np.array([\"COVID\" if p == 1 else \"normal\" for p in predicted_disease])\n",
        "\n",
        "top_30_genes = [\n",
        "    'IFI27', 'IFITM3', 'S100A8', 'MT-CYB', 'S100A9', 'TYROBP', 'IFITM1',\n",
        "    'IFI44L', 'CLU', 'CST3', 'PTPRCAP', 'RPS4Y1', 'AREG', 'RPS10', 'NKG7',\n",
        "    'MT-ATP6', 'RPS27', 'LYZ', 'JUNB', 'DDIT4', 'MT-ND3', 'RPL36A', 'TSC22D3',\n",
        "    'CXCR4', 'GZMB', 'FCER1G', 'LY6E', 'HLA-DRB5', 'VCAN', 'LGALS2'\n",
        "]\n",
        "\n",
        "available_genes = [gene for gene in top_30_genes if gene in adata_sub.var['feature_name'].values]\n",
        "adata_plot = adata_sub[:, adata_sub.var['feature_name'].isin(available_genes)].copy()\n",
        "\n",
        "adata_plot.var_names = adata_plot.var['feature_name']\n",
        "adata_plot.var_names_make_unique()\n",
        "\n",
        "sc.pp.scale(adata_plot, max_value=2.5)\n",
        "\n",
        "sc.pl.dotplot(\n",
        "    adata_plot,\n",
        "    var_names=available_genes,\n",
        "    groupby=\"disease_pred\",\n",
        "    use_raw=False, #use the scaled data in .X\n",
        "    cmap='Reds',\n",
        "    title=\"Top 30 Model-Identified Genes (Predicted COVID vs. Normal)\",\n",
        "    figsize=(18, 6) #figsizee\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTNxaK_5u9q4"
      },
      "source": [
        "**ABLATION**;\n",
        "**5 times, average, for accuracy**;\n",
        "**Novel markers for covid with deep learning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4petlk-YK0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = \"\"\n",
        "MODEL_PATH = f\"{DATA_DIR}/dual_head_epoch07.pt\"\n",
        "ADATA_PATH = f\"{DATA_DIR}/covid_2kHVG.h5ad\"\n",
        "\n",
        "#hard coded top 30 genes\n",
        "TOP_30_SHAP_GENES = [\n",
        "    'S100A8', 'MT-CYB', 'IFI27', 'IFITM1', 'IFITM3', 'S100A9', 'IFI44L', 'NKG7',\n",
        "    'PTPRCAP', 'MT-ATP6', 'RPS4Y1', 'LY6E', 'NEAT1', 'FTL', 'JUNB', 'RPS20',\n",
        "    'TSC22D3', 'TYROBP', 'IER2', 'DDIT4', 'FCER1G', 'AREG', 'PCBP2', 'RPS10',\n",
        "    'RPS29', 'CD69', 'MT2A', 'RPL36A', 'MAFB', 'SRGN'\n",
        "]\n",
        "\n",
        "#load data + model\n",
        "adata = sc.read_h5ad(ADATA_PATH)\n",
        "\n",
        "#instantiate it from path\n",
        "n_input = adata.n_vars\n",
        "n_cell = len(adata.obs['cell_type'].cat.categories)\n",
        "model = DualHeadNet(n_input, n_cell).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "#test set\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "gss_tv = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
        "train_idx, val_idx_all = next(gss_tv.split(np.arange(adata.n_obs), groups=groups))\n",
        "gss_vt = GroupShuffleSplit(test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(gss_vt.split(val_idx_all, groups=groups[val_idx_all]))\n",
        "print(f\"Using a fixed test set of {len(test_idx)} cells.\")\n",
        "\n",
        "#preparing test data\n",
        "X_test_original = adata.X[test_idx].toarray().astype(np.float32)\n",
        "y_test_disease = (adata.obs[\"disease\"].values[test_idx] != \"normal\").astype(int)\n",
        "\n",
        "#gene symbols to data indices\n",
        "print(\"Map thae gene symbols to data indices\")\n",
        "gene_symbol_to_idx = {name: i for i, name in enumerate(adata.var['feature_name'])}\n",
        "top_30_shap_indices = [gene_symbol_to_idx[gene] for gene in TOP_30_SHAP_GENES]\n",
        "print(\"Successful\")\n",
        "\n",
        "\n",
        "#eval\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X, self.y = X, y\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, i): return torch.from_numpy(self.X[i]), self.y[i]\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy(model, dataset):\n",
        "    loader = DataLoader(dataset, batch_size=2048, num_workers=0)\n",
        "    all_preds = []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        _, disease_logits = model(xb)\n",
        "        preds = (disease_logits > 0).int().cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    return accuracy_score(dataset.y, y_pred)\n",
        "\n",
        "#run 5 loop ablation experi\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" Running Ablation Accuracy Test (Disease Head)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for i in range(1):\n",
        "    loop_results = {}\n",
        "\n",
        "    #base acc\n",
        "    baseline_dataset = EvalDataset(X_test_original, y_test_disease)\n",
        "    loop_results['Baseline'] = get_accuracy(model, baseline_dataset)\n",
        "\n",
        "    #top 30 ablation\n",
        "    X_test_ablated_top = X_test_original.copy()\n",
        "    X_test_ablated_top[:, top_30_shap_indices] = 0.0\n",
        "    top_ablated_dataset = EvalDataset(X_test_ablated_top, y_test_disease)\n",
        "    loop_results['Ablated Top 30'] = get_accuracy(model, top_ablated_dataset)\n",
        "\n",
        "    #randomized 30 genes ablation\n",
        "    all_gene_indices = np.arange(adata.n_vars)\n",
        "    non_top_indices = np.setdiff1d(all_gene_indices, top_30_shap_indices)\n",
        "    random_ablation_indices = np.random.choice(non_top_indices, 30, replace=False)\n",
        "\n",
        "    X_test_ablated_rand = X_test_original.copy()\n",
        "    X_test_ablated_rand[:, random_ablation_indices] = 0.0\n",
        "    rand_ablated_dataset = EvalDataset(X_test_ablated_rand, y_test_disease)\n",
        "    loop_results['Ablated Random 30'] = get_accuracy(model, rand_ablated_dataset)\n",
        "\n",
        "    all_results.append(loop_results)\n",
        "\n",
        "    print(f\"Loop {i+1}: \"\n",
        "          f\"Baseline Acc: {loop_results['Baseline']:.4f},  \"\n",
        "          f\"Top 30 Ablated Acc: {loop_results['Ablated Top 30']:.4f},  \"\n",
        "          f\"Random 30 Ablated Acc: {loop_results['Ablated Random 30']:.4f}\")\n",
        "\n",
        "#summarize\n",
        "print(\"=\"*60)\n",
        "results_df = pd.DataFrame(all_results)\n",
        "print(\"\\nSummary of Accuracies on 1 loop\")\n",
        "print(results_df.mean().to_frame('Average Accuracy').to_markdown(floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwGIQg0JRxd-"
      },
      "outputs": [],
      "source": [
        "#statistical test\n",
        "import numpy as np, torch, pandas as pd, gc\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import wilcoxon, ttest_rel\n",
        "\n",
        "N_LOOPS   = 30\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "print(f\"Running {N_LOOPS} donorâ€‘stratified splits â€¦\")\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X, self.y = X, y\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, i): return torch.from_numpy(self.X[i]), self.y[i]\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy(model, dataset):\n",
        "    loader, preds = DataLoader(dataset, 2048), []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        _, disease_logits = model(xb)\n",
        "        preds.append((disease_logits > 0).int().cpu().numpy())\n",
        "    return accuracy_score(dataset.y, np.concatenate(preds))\n",
        "\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "baseline_accs, ablated_accs = [], []\n",
        "\n",
        "gss = GroupShuffleSplit(test_size=TEST_SIZE, n_splits=N_LOOPS, random_state=123)\n",
        "for loop, (_, test_idx) in enumerate(gss.split(np.arange(adata.n_obs), groups=groups), 1):\n",
        "    X_test = adata.X[test_idx].toarray().astype(np.float32)\n",
        "    y_test = (adata.obs[\"disease\"].values[test_idx] != \"normal\").astype(int)\n",
        "\n",
        "    baseline_accs.append(\n",
        "        get_accuracy(model, EvalDataset(X_test, y_test))\n",
        "    )\n",
        "\n",
        "    X_ab = X_test.copy()\n",
        "    X_ab[:, top_30_shap_indices] = 0.0\n",
        "    ablated_accs.append(\n",
        "        get_accuracy(model, EvalDataset(X_ab, y_test))\n",
        "    )\n",
        "\n",
        "    print(f\"Loop {loop:02d}:  Baseline {baseline_accs[-1]:.4f}  |  \"\n",
        "          f\"Ablated {ablated_accs[-1]:.4f}\")\n",
        "\n",
        "    del X_test, X_ab, y_test; gc.collect()\n",
        "\n",
        "baseline = np.array(baseline_accs)\n",
        "ablated  = np.array(ablated_accs)\n",
        "delta    = ablated - baseline\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"Descriptive statistics\")\n",
        "print(pd.DataFrame({\"Baseline\": baseline, \"Ablated\": ablated, \"Delta\": delta})\n",
        "        .describe().round(4).to_markdown())\n",
        "\n",
        "w_stat, w_p = wilcoxon(baseline, ablated, alternative='two-sided')\n",
        "print(\"\\nWilcoxon signedâ€‘rank test (paired, nonâ€‘parametric)\")\n",
        "print(f\"n = {N_LOOPS},  statistic = {w_stat:.0f},  pâ€‘value = {w_p:.3e}\")\n",
        "\n",
        "t_stat, t_p = ttest_rel(baseline, ablated)\n",
        "print(\"\\nPaired tâ€‘test (assumes normality of differences)\")\n",
        "print(f\"n = {N_LOOPS},  t = {t_stat:.3f},  pâ€‘value = {t_p:.3e}\")\n",
        "\n",
        "signif = lambda p: \"significant\" if p < 0.05 else \"not significant\"\n",
        "print(f\"\\nWilcoxon result: {signif(w_p)}  |  tâ€‘test result: {signif(t_p)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqg2cQe6l71d"
      },
      "source": [
        "ONE BY ONE CELL ABLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yULpe6_seJ3M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "\n",
        "#config\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = \"\"\n",
        "MODEL_PATH = f\"{DATA_DIR}/dual_head_epoch07.pt\"\n",
        "ADATA_PATH = f\"{DATA_DIR}/covid_2kHVG.h5ad\"\n",
        "\n",
        "#top 30 genes in an indexed array\n",
        "TOP_30_SHAP_GENES = [\n",
        "    'S100A8', 'MT-CYB', 'IFI27', 'IFITM1', 'IFITM3', 'S100A9', 'IFI44L', 'NKG7',\n",
        "    'PTPRCAP', 'MT-ATP6', 'RPS4Y1', 'LY6E', 'NEAT1', 'FTL', 'JUNB', 'RPS20',\n",
        "    'TSC22D3', 'TYROBP', 'IER2', 'DDIT4', 'FCER1G', 'AREG', 'PCBP2', 'RPS10',\n",
        "    'RPS29', 'CD69', 'MT2A', 'RPL36A', 'MAFB', 'SRGN'\n",
        "]\n",
        "\n",
        "#data loader + model\n",
        "adata = sc.read_h5ad(ADATA_PATH)\n",
        "n_input = adata.n_vars\n",
        "n_cell = len(adata.obs['cell_type'].cat.categories)\n",
        "#dual head net in previous\n",
        "model = DualHeadNet(n_input, n_cell).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"loaded model\")\n",
        "\n",
        "#define test set\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "gss_tv = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
        "train_idx, val_idx_all = next(gss_tv.split(np.arange(adata.n_obs), groups=groups))\n",
        "gss_vt = GroupShuffleSplit(test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(gss_vt.split(val_idx_all, groups=groups[val_idx_all]))\n",
        "print(f\"Using a fixed test set of {len(test_idx)} cells.\")\n",
        "\n",
        "X_test_original = adata.X[test_idx].toarray().astype(np.float32)\n",
        "y_test_disease = (adata.obs[\"disease\"].values[test_idx] != \"normal\").astype(int)\n",
        "\n",
        "gene_symbol_to_idx = {name: i for i, name in enumerate(adata.var['feature_name'])}\n",
        "top_30_shap_indices = [gene_symbol_to_idx[gene] for gene in TOP_30_SHAP_GENES]\n",
        "print(\"Gene mapping successful for top 30 genes.\")\n",
        "#eval for accuracy\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, X, y): self.X, self.y = X, y\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, i): return torch.from_numpy(self.X[i]), self.y[i]\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_accuracy(model, dataset):\n",
        "    loader = DataLoader(dataset, batch_size=2048, num_workers=0)\n",
        "    all_preds, all_true = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        _, disease_logits = model(xb)\n",
        "        #get final 0/1 predictions\n",
        "        preds = (disease_logits > 0).int().cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_true.append(yb.numpy())\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_true)\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "#run gene ablation and print results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"  Running Gene-by-Gene Ablation Accuracy Test\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "#get + print baseline performance\n",
        "baseline_dataset = EvalDataset(X_test_original, y_test_disease)\n",
        "baseline_accuracy = get_accuracy(model, baseline_dataset)\n",
        "print(f\"Baseline accuracy (no genes ablated): {baseline_accuracy:.6f}\\n\")\n",
        "\n",
        "#loop thru each gene\n",
        "for gene_name, gene_idx in zip(TOP_30_SHAP_GENES, top_30_shap_indices):\n",
        "\n",
        "    #create copy and single out zero gene\n",
        "    X_test_ablated_single = X_test_original.copy()\n",
        "    X_test_ablated_single[:, gene_idx] = 0.0\n",
        "\n",
        "    #eval on performance\n",
        "    ablated_dataset = EvalDataset(X_test_ablated_single, y_test_disease)\n",
        "    ablated_accuracy = get_accuracy(model, ablated_dataset)\n",
        "\n",
        "    #print paired output\n",
        "    print(f\"Ablating gene: {gene_name}\")\n",
        "    print(f\"Accuracy after ablation: {ablated_accuracy:.6f}\\n\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Gene-by-gene ablation complete.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlssnBJLuKV-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = \"\"\n",
        "ADATA_PATH = f\"{DATA_DIR}/covid_2kHVG.h5ad\"\n",
        "EPOCHS = 20\n",
        "NUM_RUNS = 5\n",
        "RANDOM_SEEDS = [] #choose random seeds\n",
        "\n",
        "#define original model\n",
        "class DualHeadNet(nn.Module):\n",
        "    def __init__(self, n_input, n_cell):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(n_input, 1024), nn.ReLU(), nn.BatchNorm1d(1024), nn.Dropout(0.25),\n",
        "            nn.Linear(1024, 512), nn.ReLU(), nn.BatchNorm1d(512),  nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "        )\n",
        "        self.head_cell    = nn.Linear(256, n_cell)\n",
        "        self.head_disease = nn.Linear(256, 1)\n",
        "    def forward(self, x):\n",
        "        z = self.shared(x)\n",
        "        return self.head_cell(z), self.head_disease(z).squeeze(-1)\n",
        "\n",
        "class CSRDataset(Dataset):\n",
        "    def __init__(self, csr_mat, y1, y2, mask):\n",
        "        self.csr, self.y1, self.y2 = csr_mat, y1, y2\n",
        "        self.indices = torch.where(mask)[0]\n",
        "    def __len__(self): return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = int(self.indices[i])\n",
        "        x = torch.as_tensor(self.csr[idx].toarray(), dtype=torch.float32).squeeze(0)\n",
        "        return x, self.y1[idx], self.y2[idx]\n",
        "\n",
        "#seed trainer\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def run_epoch(model, loader, optimizer, scaler, loss_cell_fn, loss_disease_fn, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss = corr_cell = corr_dis = nsamp = 0\n",
        "    from tqdm.auto import tqdm\n",
        "    loader_tqdm = tqdm(loader, desc=\"Training\" if train else \"Validation\", leave=False)\n",
        "    for xb, yb_cell, yb_dis in loader_tqdm:\n",
        "        xb, yb_cell, yb_dis = xb.to(DEVICE), yb_cell.to(DEVICE), yb_dis.to(DEVICE)\n",
        "        with torch.set_grad_enabled(train), autocast(device_type=DEVICE.type):\n",
        "            logits_cell, logit_dis = model(xb)\n",
        "            loss = loss_cell_fn(logits_cell, yb_cell) + loss_disease_fn(logit_dis, yb_dis)\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "        batch = xb.size(0)\n",
        "        total_loss += loss.item() * batch\n",
        "        corr_cell  += (logits_cell.argmax(1) == yb_cell).sum().item()\n",
        "        corr_dis   += ((logit_dis > 0).int() == yb_dis.int()).sum().item()\n",
        "        nsamp      += batch\n",
        "    return (total_loss / nsamp, corr_cell / nsamp, corr_dis / nsamp)\n",
        "\n",
        "#t-loop\n",
        "adata = sc.read_h5ad(ADATA_PATH)\n",
        "groups = adata.obs[\"donor_id\"].values\n",
        "y_cell_full = torch.tensor(adata.obs[\"cell_type\"].astype(\"category\").cat.codes.values, dtype=torch.long)\n",
        "y_dis_full  = torch.tensor((adata.obs[\"disease\"] != \"normal\").astype(int).values, dtype=torch.float32)\n",
        "\n",
        "final_val_acc_disease = []\n",
        "final_val_acc_cell = []\n",
        "\n",
        "for i, seed in enumerate(RANDOM_SEEDS):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"  Start run {i+1}/{NUM_RUNS} with Random Seed: {seed}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "    train_idx, val_idx = next(gss.split(np.arange(adata.n_obs), groups=groups))\n",
        "    train_mask = torch.from_numpy(np.isin(np.arange(adata.n_obs), train_idx))\n",
        "    val_mask   = torch.from_numpy(np.isin(np.arange(adata.n_obs), val_idx))\n",
        "\n",
        "    train_ds = CSRDataset(adata.X, y_cell_full, y_dis_full, train_mask)\n",
        "    val_ds   = CSRDataset(adata.X, y_cell_full, y_dis_full, val_mask)\n",
        "    train_dl = DataLoader(train_ds, batch_size=512, shuffle=True, num_workers=0)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=1024, num_workers=0)\n",
        "\n",
        "    model = DualHeadNet(adata.n_vars, len(adata.obs['cell_type'].cat.categories)).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
        "\n",
        "    #Use weighted loss\n",
        "    counts = np.bincount(y_cell_full[train_idx].numpy(), minlength=len(adata.obs['cell_type'].cat.categories))\n",
        "\n",
        "    #fix divide by zero\n",
        "    w_cell = torch.tensor(1.0 / (counts + 1e-9) * (len(counts) / (1.0 / (counts + 1e-9)).sum()), dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "    loss_cell_fn = nn.CrossEntropyLoss(weight=w_cell)\n",
        "    loss_disease_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss, tr_acc_cell, tr_acc_dis = run_epoch(model, train_dl, optimizer, scaler, loss_cell_fn, loss_disease_fn, train=True)\n",
        "        va_loss, va_acc_cell, va_acc_dis = run_epoch(model, val_dl, optimizer, scaler, loss_cell_fn, loss_disease_fn, train=False)\n",
        "        print(f\"Epoch {epoch:02d} | Val Loss {va_loss:.4f} | Val Cell Acc: {va_acc_cell:.4f} | Val Disease Acc: {va_acc_dis:.4f}\")\n",
        "\n",
        "    final_val_acc_disease.append(va_acc_dis)\n",
        "    final_val_acc_cell.append(va_acc_cell)\n",
        "    print(f\"--- Run {i+1} Complete ---\")\n",
        "    print(f\"Final Cell Validation Accuracy:    {va_acc_cell:.4f}\")\n",
        "    print(f\"Final Disease Validation Accuracy: {va_acc_dis:.4f}\")\n",
        "\n",
        "#final summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  Final Results Across All Runs\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_data = {\n",
        "    'Run': [f\"Run {i+1} (Seed {seed})\" for i, seed in enumerate(RANDOM_SEEDS)],\n",
        "    'Cell Head Accuracy': final_val_acc_cell,\n",
        "    'Disease Head Accuracy': final_val_acc_disease\n",
        "}\n",
        "results_df = pd.DataFrame(results_data)\n",
        "print(results_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "\n",
        "avg_acc_cell = np.mean(final_val_acc_cell)\n",
        "std_acc_cell = np.std(final_val_acc_cell)\n",
        "avg_acc_disease = np.mean(final_val_acc_disease)\n",
        "std_acc_disease = np.std(final_val_acc_disease)\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(f\"Cell Head    | Average Validation Accuracy: {avg_acc_cell:.4f} (Std Dev: {std_acc_cell:.4f})\")\n",
        "print(f\"Disease Head | Average Validation Accuracy: {avg_acc_disease:.4f} (Std Dev: {std_acc_disease:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDNulIaHD_WO"
      },
      "outputs": [],
      "source": [
        "#top_k\n",
        "import torch, shap, gc, numpy as np, os, pickle\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEVICE      = next(model.parameters()).device\n",
        "TOP_K       = 30\n",
        "DRIVE_DIR   = \"\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "MARKER_PKL  = os.path.join(DRIVE_DIR, f\"topk_by_class_{TOP_K}.pkl\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if os.path.exists(MARKER_PKL):\n",
        "    with open(MARKER_PKL, \"rb\") as f:\n",
        "        topk_by_class = pickle.load(f)\n",
        "    print(f\"Loaded cached markers from Â«{MARKER_PKL}Â»\")\n",
        "else:\n",
        "    topk_by_class = {}\n",
        "    print(f\"Computing SHAP topâ€‘{TOP_K} genes for {n_cell} classes â€¦\")\n",
        "    for cls in tqdm(range(n_cell)):\n",
        "        #wrapper that logics\n",
        "        class Mod(torch.nn.Module):\n",
        "            def __init__(self, shared, head, idx):\n",
        "                super().__init__(); self.s, self.h, self.i = shared, head, idx\n",
        "            def forward(self, x): return self.h(self.s(x))[:, self.i].unsqueeze(1)\n",
        "\n",
        "        ref_idx = np.random.choice(len(train_idx), 800, replace=False)\n",
        "        explainer = shap.DeepExplainer(\n",
        "            Mod(model.shared, model.head_cell, cls).to(DEVICE),\n",
        "            torch.tensor(\n",
        "                adata.X[train_idx[ref_idx]].toarray().astype(\"float32\"),\n",
        "                device=DEVICE\n",
        "            )\n",
        "        )\n",
        "\n",
        "        rows = np.where(y_cell.numpy() == cls)[0]\n",
        "        rows = np.random.choice(rows, min(200, len(rows)), replace=False)\n",
        "        sv   = explainer.shap_values(\n",
        "                torch.tensor(adata.X[rows].toarray().astype(\"float32\"),\n",
        "                             device=DEVICE))[0]\n",
        "\n",
        "        mean_abs = np.mean(np.abs(sv), axis=0)\n",
        "        topk_by_class[cls] = np.argsort(-mean_abs)[:TOP_K].tolist()\n",
        "\n",
        "        del explainer, sv; gc.collect()\n",
        "\n",
        "    with open(MARKER_PKL, \"wb\") as f:\n",
        "        pickle.dump(topk_by_class, f)\n",
        "    print(f\"Saved marker list â†’ Â«{MARKER_PKL}Â»\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnZLS9wIMrkr"
      },
      "outputs": [],
      "source": [
        "#ablation\n",
        "import torch, pickle, numpy as np, pandas as pd, os\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "DEVICE     = next(model.parameters()).device\n",
        "DRIVE_DIR  = \"\"\n",
        "MARKER_PKL = os.path.join(DRIVE_DIR, \"topk_by_class_30.pkl\")  #same TOP_K as CellÂ A\n",
        "RESULT_CSV = os.path.join(DRIVE_DIR, \"cell_ablation_results.csv\")\n",
        "\n",
        "#loaded markers\n",
        "with open(MARKER_PKL, \"rb\") as f:\n",
        "    topk_by_class = pickle.load(f)\n",
        "\n",
        "#helper dataset\n",
        "class GeneAblDataset(Dataset):\n",
        "    def __init__(self, mat): self.X = mat\n",
        "    def __len__(self):       return self.X.shape[0]\n",
        "    def __getitem__(self, i): return torch.from_numpy(self.X[i])\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(mat):\n",
        "    loader, out = DataLoader(GeneAblDataset(mat), 2048), []\n",
        "    for xb in loader:\n",
        "        out.append(model(xb.to(DEVICE))[0].argmax(1).cpu())\n",
        "    return torch.cat(out).numpy()\n",
        "\n",
        "def safe_acc(cm):\n",
        "    support = cm.sum(1)\n",
        "    acc     = np.full_like(support, np.nan, dtype=float)\n",
        "    mask    = support > 0\n",
        "    acc[mask] = np.diag(cm)[mask] / support[mask]\n",
        "    return acc\n",
        "#baseline\n",
        "X_test = adata.X[test_idx].toarray().astype(\"float32\")\n",
        "y_test = y_cell.numpy()[test_idx]\n",
        "\n",
        "baseline_pred = predict(X_test)\n",
        "baseline_cm   = confusion_matrix(y_test, baseline_pred,\n",
        "                                 labels=np.arange(n_cell))\n",
        "baseline_acc  = safe_acc(baseline_cm)\n",
        "\n",
        "#abl loop\n",
        "results = []\n",
        "\n",
        "for cls in tqdm(range(n_cell), desc=\"Ablating per class\"):\n",
        "    X_ab = X_test.copy()\n",
        "    X_ab[:, topk_by_class[cls]] = 0.0\n",
        "\n",
        "    ab_pred = predict(X_ab)\n",
        "    ab_cm   = confusion_matrix(y_test, ab_pred,\n",
        "                               labels=np.arange(n_cell))\n",
        "    ab_acc  = safe_acc(ab_cm)\n",
        "\n",
        "    results.append({\n",
        "        \"class_idx\":    cls,\n",
        "        \"class_name\":   cell_labels[cls],\n",
        "        \"baseline_acc\": baseline_acc[cls],\n",
        "        \"ablated_acc\":  ab_acc[cls],\n",
        "        \"delta\":        ab_acc[cls] - baseline_acc[cls]\n",
        "    })\n",
        "\n",
        "df = (pd.DataFrame(results)\n",
        "        .dropna()\n",
        "        .sort_values(\"delta\"))\n",
        "\n",
        "display(df)        #biggest drops\n",
        "\n",
        "#save full table for later plots / reports\n",
        "df.to_csv(RESULT_CSV, index=False)\n",
        "print(f\"\\nFull results saved â†’ Â«{RESULT_CSV}Â»\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMSkzrcYvguWLhysAyCXDxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
